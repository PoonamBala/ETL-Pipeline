# ETL-Pipeline

AIM:
To build an efficient ETL Pipeline which Extracts, Transforms and Loads data from a destination to another. It must consist of a web dashboard which performs visualization on data and helps reporting errors occurring in the pipeline.

Solution 1:
-Extracting data from SQL Server and loading data into Postgres
-Automating the pipeline using Airflow (using TaskFlow API) and using DAG
-Performing Incremental Data loading using Destination Change Comparison technique
-Alteryx- data visualization tool that works excellently with ETL

SOLUTION 2 FINAL: Using Cloud

a. Tools:
DBEAVER-To create and manage databases across a wide range of database management systems.
REDSHIFT-Large scale data storage and analysis, and is frequently used to perform large database migrations.
DATA PIPELINE-helps you reliably process and move data between different AWS compute and storage services, as well as on-premises data sources, at specified intervals.
GLUE-Uses ETL jobs to extract data from a combination of other Amazon Web Services and incorporates it into data lakes and data warehouses.
ATHENA-Performs interactive queries in the web-based cloud storage service, Amazon Simple Storage Service (S3). Athena is used with large-scale data sets. Amazon S3 is designed for online backup and archiving of data
PYSPARK-A standard ETL tool like PySpark, supports all basic data transformation features like sorting, mapping, joins, operations, etc.
QUICKSIGHT-Use to deliver easy-to-understand insights to the people who you work with, wherever they are. Amazon QuickSight connects to your data in the cloud and combines data from many different sources.
CLOUDWATCH-An AWS monitoring service you can use to monitor your applications, services, and resources. 

b. Steps:
See PPT

c. Website
1. Built using ReactJS
2. Deployed in Netlify
3. Configured to a new domain 
4. Added the domain in Domain and embedding section of the AWS page using NodeJS




By:
Poonam,Janani,Allen,Aakash
